---
title: "R Notebook: Credit Card Fraud Classification"
output: html_notebook
---


```{r}
#install.packages('tidyverse')
library(tidyverse)
library(lubridate)
library(randomForest)

raw_data <- read_csv('creditcard.csv')
View(raw_data)
```


When in the exploratory data analysis stage of any project it is recommended that you visualise the data to look for any obvious patterns or outliers in the data. Once the dataset was loaded into the R environment, the author plotted Time on the x axis, the transaction amount on the y axis and then coloured each observation based on its class.

Having loaded in the dataframe, the first discrepency the author noticed was the Class column was of type integer. As this column represents the validity of the transaction ,fraudulent or not, this column was then changed to a column of type factor.

When originally an integer column, the colours where displayed as ranges instead of distinct colours in the plot. This made locating the fraudulent transactions quite tricky. Having resolved the data type of the Class column, it was still difficult to see the fraudulent transactions as there were far more valid transactions than fraudulent transactions.

Therefore adding the alpha aesthetic with the colour aesthetic provided a better view of the fraudulent transactions.

```{r}
ggplot(data = raw_data) + geom_point( mapping = aes(x = Time, y = Amount, color = Class)) # Class column as int

raw_data$Class = factor(raw_data$Class)

ggplot(data = raw_data) + geom_point( mapping = aes(x = Time, y = Amount, color = Class) , na.rm = TRUE) # Class column as factor

graph <- ggplot(data = raw_data) + geom_point( mapping = aes(x = Time, y = Amount, color = Class, alpha = Class), na.rm = TRUE) # Dynamic alpha level

png('Figure 1.png')
print(graph)
dev.off()

(num_valid <- raw_data %>%
  group_by(Class) %>%
  count()) # count observations for each class of transactions
  
```

As you can see when the dataframe has been grouped by the class of the transaction and a basic count has been made for the number of observations of each class, it is evident the dataset is highly unbalanced. There are only 492 fraudulent transactions compared to 284315 valid transactions. 

Also note the first plot alerts us to the fact that there is one row that contains an NA value which has been removed from the plot by default. To avoid this warning you need to explicitly remove the *na* values in the geom function.

The next step was to get some basic summary statistics on the fraudulent transactions. To do this the author filtered the dataset so the summary statistics would be based on the fraudulent transactions only.

```{r}
fraud <- filter(raw_data, Class == 1)
fraud

summary(fraud$Amount)
summary(fraud$Time)

fraud %>%
  filter(Amount < .5) %>%
  count()

fraud %>%
  filter(Amount < .1) %>%
  count()


```

Having looked at the summary statistics its interesting to note that the smallest transaction is $0.00 which has been classified as fraud. This could be an anomaly within the data. Again using the same principle as before visualising the fraudulent data may provide some correlations with other data from the dataset.

```{r}
fraud$Amount

(graph <- ggplot(data = fraud) + geom_point( mapping = aes(x = Time, y = Amount)))

png('Figure 2.png')
print(graph)
dev.off()
```

Having looked at the transaction amounts, many of the fraudulent transactions are no significantly large. Unfortunately the time is given in an unusual manner in this dataset. The times are recorded in seconds from the first transaction. This is not very informative in terms of the values on the x axis. To solve this a function was created that took the fraud dataframe and generates a 3 graphs where the x axis goes from minutes to hours to days. 

```{r}
generate_times <- function(fraud_data)
{
  fraud_data <- fraud_data %>%
    mutate(mins = Time / 60)%>%
    mutate(hours = Time / 60 / 60) %>%
    mutate(days = Time / 60 / 60 / 24)
  return(fraud_data)
}

generate_graphs <- function(frauds, x, y)
{
  return (ggplot(data = frauds) + geom_point( mapping = aes(x ,  y)) )
}

fraud_times <- generate_times(fraud)

(graph_minutes <- generate_graphs(fraud_times, fraud_times$mins, fraud_times$Amount) + xlab('Minutes') + ylab('Amount'))
(graph_hours <- generate_graphs(fraud_times, fraud_times$hours, fraud_times$Amount) + xlab('Hours') + ylab('Amount'))
(graph_days <- generate_graphs(fraud_times, fraud_times$days, fraud_times$Amount) + xlab('Days') + ylab('Amount'))

```

Again, even when the time data is change to minutes, hours and days, the majority of the values have a transaction amount that is still quite low. Looking at the x axis in terms of hours, we will now limit the y axis to an arbitrary value to see the dispersion of the lower transactions.

```{r}
(graph <- ggplot(data = fraud_times) + geom_point( mapping = aes(mins ,  Amount )) + ylim(0,10))
png('Figure 3.png')
print(graph)
dev.off()

(graph <- ggplot(data = fraud_times) + geom_point( mapping = aes(mins ,  Amount )) + ylim(10, max(fraud_times$Amount) ))
png('Figure 3.1.png')
print(graph)
dev.off()
(ggplot(data = fraud_times) + geom_point( mapping = aes(mins ,  Amount )) + coord_cartesian(ylim = c(10, 250))
 )
```

Having limited the maximum transaction amount to $10 in the first plot, it is evident that a large proportion of the transactions are less than $5. In the second plot the author look at all the transactions greater than $10 which shows us a large proportion of the data points falling between $10 and $250. In the third plot the author "zooms" in on the values that fall between $10 and $250. Unfortunately there appears to be no direct correlation between the time and the amount of the transaction. This is clarified when we look at the correlation coefficient between time in minutes and the transaction amount.

```{r}
?cor

cor(fraud_times$mins, fraud_times$Amount)
```

As we can see the correlation between the time in minutes and transactions amount is very low.

As all the variables in this dataset are in a numeric format we need to normalize them before working with any machine learning algorithms. Also as the dataset is highly unbalanced and contains approximately 280,000 rows of data, it is suggested that only a sample of the dataset be used to keep computing time to a minimum.

Therefore it is suggested that the experiment will be carried out on a sample of the dataset containing all the fraudulent transactions and 10% of the valid transactions. Note, to make the models as accurate as possible all fraudulent transactions will be included in the sample.

```{r}
non_fraud <- raw_data %>%
  filter(Class != 1)

set.seed(1)
non_fraud_sample <- sample_frac(non_fraud, .1, replace = FALSE )
non_fraud_sample

(count(non_fraud_sample))

#Combine fraudulent transactions with valid transactions.
 
(sample_data <- rbind(non_fraud_sample,fraud))
(count(sample_data))
```

Having created our sample the data needs to be normalised before use in the machine learning algorithms.

```{r}
#x - min(x) / max(x) - min(x)

raw_data <- na.omit(raw_data)

raw_data$Time = as.double(raw_data$Time)

normalise_data <- function(temporary_data)
{
  for (column in names(temporary_data[2:30]) )
    {
        max_value <- max(temporary_data[[column]])
        min_value <- min(temporary_data[[column]])
  
  
        for (value in 1: length(temporary_data[[column]]) )
        {
          
          temporary_data[[column]][value] <- (temporary_data[[column]][value] - min_value) / (max_value - min_value)
          
        }
    }
  return(temporary_data)
}
system.time( {normalised <- normalise_data(sample_data) })
normalised
```

Although only 10% of the dataset is being used it takes approximately 3 minutes to normalise all the data except for the class column as its a factor. The following webpage provide the author with the necessary documentation and examples for a stratified split of the data: *http://topepo.github.io/caret/data-splitting.html*

```{r}
#install.packages('caret' ,dependencies = TRUE)
library(caret)

set.seed(2)
train_index <- createDataPartition(normalised$Class, times=1, p=.5, list = FALSE)
XTrain <- normalised[ train_index,]

TotalTest  <- normalised[-train_index,]
test_index <- createDataPartition(TotalTest$Class, times=1, p=.5, list = FALSE)
XTest <- normalised[ test_index,]

independent_Test  <- normalised[-test_index,]

```

The K Nearest Neighbor classifier is one of the simplest classification algorithms to understand. In essence the algorithm works by storing the training set and then finding whats the closest training observation to the incoming test observation. The test observation is then categorised as the class of the training observation. This is the case if k = 1. If K = 5 for example, the algorithm works by locating 5 closest training observations to the test observation and assigns it the class that occurs most in the 5 neighbours.

```{r}
basic_knn <- function(XTrain, XTest)
{
  #install.packages('class')
  #install.packages('gmodels')
  library(gmodels)
  library(class)
  
  set.seed(4)
  model <- train(as.matrix(XTrain[2:30]), as.matrix(XTrain[31]), method = 'knn',)
  #model stores the predicted output for each data point as well as a proportion of votes for the winning class.
  
  return(model)
}

system.time( {model <- basic_knn(XTrain, XTest) })

predicted_classes <- predict(model, newdata = as.matrix(XTest[2:30]))

(results <- confusionMatrix(as.matrix(XTest[31]), predicted_classes,  positive='1'  ))

```

The CrossTable function gives a description of the predictions but does not actually give an accuracy score for the model. This has to be calculated manually unfortunately. In this case the accuracy score of the model is approximately 99.6%. The accuracy is calculated by adding the total number of correct predictions for valid transactions with the total number of correct predictions for fraudulent transactions and then dividing that by the total number of observations. Note this accurracy is achieved without including the Time column.

```{r}
?train
knn_cv <- function(XTrain)
{
  training_control <- trainControl(method ='repeatedcv', repeats = 5)
  
  set.seed(5)
  knn_model <- train(as.matrix(XTrain[2:30]), as.matrix(XTrain[31]), trControl = training_control, method = 'knn')
  
  return(knn_model)
}
```

Using the caret package for KNN is considerably faster that R's base KNN class. This may be due to 10 fold cross-validation which splits the training sample into 10 samples. One of these samples is then used as a test set. Therefore 90% of the data is used for training. This process is repeated five times.

```{r}

system.time( {knn_model <- knn_cv(XTrain) })

```

Having printed the model summary we can see that the training found K = 5 as the optimum setting.

```{r}
predicted_classes <- predict(knn_model, newdata = as.matrix(XTest[2:30])) # predict classes for the test observations
#predicted_classes <- list(predicted_classes)

#typeof(predicted_classes)
#typeof(XTest[31])

(results <- confusionMatrix(as.matrix(XTest[31]), predicted_classes,  positive='1'  ))
```

As you can see from the summary the model accuracy with the test set is approximately 99.6% which is quite impressive.

Sensitivity: ability to detect true positive rate
Specificity: ability to detect true negative rate.

As you can see from the summary, the specificity is only 80%, meaning only 80% of the true fraudulent transactions were predicted as being fraudulent. This may be due to the lack of fraudulent transactions in the samples. Alternatively 99.9% of the non-fraudulent transactions were correctly classified as being non-fraudulent. In this case where we are trying to correctly identify fraudulent transactions it would be better to have a higher specificity. A lower sensitivity would still be quite reasonable.

It would be more reasonable for a financial institution to missclassify non-fraudulent transactions as fraudulent compared to missclassifying fraudulent transactions as non-fraudulent.

```{r}

```

Logistic Regression.

```{r}
?glm

logistic <- function(XTrain)
{
  start_time <- Sys.time()
  
  if(length(XTrain) == 31)
    {
      XTrain<- XTrain[2:31]
    }
  
  #logistic_model <- glm(Class ~ ., data = XTrain, family = binomial(link = 'logit'))
  logistic_model <- train(Class ~ ., data = XTrain, method = 'glm', family = binomial(link = 'logit') )
  
  end_time <- Sys.time()
  
  total_execution <- end_time - start_time
  
  cat('\nThe total time to train the model was: ', total_execution)
  
  summary(logistic_model)
  
  return(logistic_model)

}

system.time( {logistic_model <- logistic(XTrain)})

logistic_predicted <- predict(logistic_model, newdata = XTest[2:30] ) # if you use the parameter type = response, it means the output is returned as P(Y =1 | X) so you need to convert to probabilites to 0 or 1 using a threshold like below
  
  
  # need to convert probabilities to 0 or 1 using a threshold
#logistic_predicted[logistic_predicted > .5] = 1
#logistic_predicted[logistic_predicted < .5] = 0
  
(confusionMatrix(as.matrix(XTest[31]), logistic_predicted, positive='1' ))


```

As you can see from the p-values associated with the coefficients, some predictors are more statistically significant than others. Despite this, the trained model will now be validated against the test set. Following this the parameters used in the training model will be reduced to only included the variables that were statistically significant. Generally anything with a p-value of .05 or less is considered statistically significant which is the criteria that will be used to improve the model.

With one run of logistic regression on the training data followed by the model being tested the accuracy of the predictions is 99.64%



```{r}
logistic_cv <- function(XTrain)
{
  library(gmodels)
  library(class)
  
  training_control <- trainControl(method ='repeatedcv', repeats = 5)
  
  set.seed(5)
  log_model <- train(Class ~ ., data = XTrain, trControl = training_control, method = 'glm', family = binomial(link = 'logit') )
  
  return(log_model)
}

system.time( {log_model <- logistic_cv(XTrain) })

```

Using repeated cross validation with 5 repeats and 10 folds per repeat. This model now needs to be validated against XTest.

```{r}
logistic_predicted <- predict(log_model, newdata = as.matrix(XTest[1:30]) ) # type = response means the output is returned as P(Y =1 | X)

#logistic_predicted[logistic_predicted > .5] = 1
#logistic_predicted[logistic_predicted < .5] = 0


(confusionMatrix(as.matrix(XTest[31]), as.matrix(logistic_predicted) , positive='1' ))
```

As you can see the accuracy using cross validation is 99.64% which results in no change from the original dataset. This may be due to the level of accuracy in the existing model which is already extremely high. The closer the model accuracy is to 100% the harder it is to improve the model.

```{r}
?randomForest
basic_RF <- function(XTrain)
{
  model <- randomForest(Class ~ ., data = XTrain[2:31], ntree=100 ) 
  return(model)
}

system.time( {RF <- basic_RF(XTrain)})


```



```{r}
?predict
predictions = predict(RF, newdata = XTest[2:30])
confusionMatrix(as.matrix(XTest[31]), as.matrix(predictions), positive='1' )



```



```{r}
RF_cv <- function(Xtrain)
{
   training_control <- trainControl(method ='repeatedcv', repeats = 5)
  
  set.seed(5)
  rf_model <- train(Class ~ ., data = XTrain[2:31], trControl = training_control, method = 'rf', ntree = 10 )
  
  return(rf_model)
}

system.time( {rf_model <- RF_cv(Xtrain)})


rf_predicted <- predict(rf_model, newdata = as.matrix(XTest[2:30]) ) 


(confusionMatrix(as.matrix(XTest[31]), as.matrix(rf_predicted) , positive='1'  ))
```

**Independent Test Set**

```{r}
CrossTable(as.matrix(independent_Test[31]), y = model)
```

```{r}
logistic_predicted <- predict(logistic_model, newdata = independent_Test[2:30] ) # if you use the parameter type = response, it means the output is returned as P(Y =1 | X) so you need to convert to probabilites to 0 or 1 using a threshold like below
  
(confusionMatrix(logistic_predicted, as.matrix(independent_Test[31]), positive='1' ))
```

```{r}
predictions = predict(RF, newdata = independent_Test[2:30])
confusionMatrix(as.matrix(predictions), as.matrix(independent_Test[31]), positive='1' )
```

**Cross Validated Models**

```{r}
predicted_classes <- predict(knn_model, newdata = as.matrix(independent_Test[2:30])) # predict classes for the test observations

(results <- confusionMatrix(predicted_classes, as.matrix(independent_Test[31]) , positive='1'  ))
```

```{r}
logistic_predicted <- predict(log_model, newdata = as.matrix(independent_Test[1:30]) ) # type = response means the output is returned as P(Y =1 | X)

(confusionMatrix(as.matrix(logistic_predicted), as.matrix(independent_Test[31]) , positive='1' ))
```

```{r}
rf_predicted <- predict(rf_model, newdata = as.matrix(independent_Test[2:30]) ) 


(confusionMatrix(as.matrix(rf_predicted), as.matrix(independent_Test[31]) , positive='1'  ))
```

